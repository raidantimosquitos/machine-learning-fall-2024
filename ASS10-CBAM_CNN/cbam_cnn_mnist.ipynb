{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding CBAM (Convolutional Block Attention Module) and CNN (Convolutional Neural Network) for classifying the MNIST hand-digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define CBAM Pytorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Convolutional Block Attention Module (CBAM)\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = self.sigmoid(avg_out + max_out)\n",
    "        return out\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.sigmoid(self.conv1(out))\n",
    "        return out\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ca = ChannelAttention(in_planes)\n",
    "        self.sa = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.ca(x)\n",
    "        x = x * self.sa(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the CNN model (which uses the previously defined CBAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model with CBAM\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3)\n",
    "        self.cbam1 = CBAM(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.cbam2 = CBAM(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3)\n",
    "        self.cbam3 = CBAM(256)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(torch.max_pool2d(self.conv1(x), 2))\n",
    "        out = self.cbam1(out)\n",
    "        out = torch.relu(torch.max_pool2d(self.conv2(out), 2))\n",
    "        out = self.cbam2(out)\n",
    "        out = torch.relu(torch.max_pool2d(self.conv3(out), 2))\n",
    "        out = self.cbam3(out)\n",
    "        out = out.flatten(1)  # Flatten the tensor\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=False, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create dataloaders from the previous datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model, loss function and optimizer initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will train with device cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (cbam1): CBAM(\n",
       "    (ca): ChannelAttention(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "      (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu1): ReLU()\n",
       "      (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (sa): SpatialAttention(\n",
       "      (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (cbam2): CBAM(\n",
       "    (ca): ChannelAttention(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "      (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu1): ReLU()\n",
       "      (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (sa): SpatialAttention(\n",
       "      (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (cbam3): CBAM(\n",
       "    (ca): ChannelAttention(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "      (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu1): ReLU()\n",
       "      (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (sa): SpatialAttention(\n",
       "      (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Will train with device {device}')\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/938], Loss: 0.5849\n",
      "Epoch [1/10], Step [200/938], Loss: 0.2238\n",
      "Epoch [1/10], Step [300/938], Loss: 0.1793\n",
      "Epoch [1/10], Step [400/938], Loss: 0.2797\n",
      "Epoch [1/10], Step [500/938], Loss: 0.1381\n",
      "Epoch [1/10], Step [600/938], Loss: 0.1300\n",
      "Epoch [1/10], Step [700/938], Loss: 0.0552\n",
      "Epoch [1/10], Step [800/938], Loss: 0.0626\n",
      "Epoch [1/10], Step [900/938], Loss: 0.0576\n",
      "Epoch [2/10], Step [100/938], Loss: 0.0914\n",
      "Epoch [2/10], Step [200/938], Loss: 0.1597\n",
      "Epoch [2/10], Step [300/938], Loss: 0.0597\n",
      "Epoch [2/10], Step [400/938], Loss: 0.1022\n",
      "Epoch [2/10], Step [500/938], Loss: 0.0636\n",
      "Epoch [2/10], Step [600/938], Loss: 0.1937\n",
      "Epoch [2/10], Step [700/938], Loss: 0.0701\n",
      "Epoch [2/10], Step [800/938], Loss: 0.0433\n",
      "Epoch [2/10], Step [900/938], Loss: 0.0345\n",
      "Epoch [3/10], Step [100/938], Loss: 0.0278\n",
      "Epoch [3/10], Step [200/938], Loss: 0.0126\n",
      "Epoch [3/10], Step [300/938], Loss: 0.0169\n",
      "Epoch [3/10], Step [400/938], Loss: 0.0268\n",
      "Epoch [3/10], Step [500/938], Loss: 0.0352\n",
      "Epoch [3/10], Step [600/938], Loss: 0.1072\n",
      "Epoch [3/10], Step [700/938], Loss: 0.0407\n",
      "Epoch [3/10], Step [800/938], Loss: 0.0024\n",
      "Epoch [3/10], Step [900/938], Loss: 0.0085\n",
      "Epoch [4/10], Step [100/938], Loss: 0.0151\n",
      "Epoch [4/10], Step [200/938], Loss: 0.0033\n",
      "Epoch [4/10], Step [300/938], Loss: 0.0175\n",
      "Epoch [4/10], Step [400/938], Loss: 0.0114\n",
      "Epoch [4/10], Step [500/938], Loss: 0.0624\n",
      "Epoch [4/10], Step [600/938], Loss: 0.0102\n",
      "Epoch [4/10], Step [700/938], Loss: 0.0305\n",
      "Epoch [4/10], Step [800/938], Loss: 0.0123\n",
      "Epoch [4/10], Step [900/938], Loss: 0.0271\n",
      "Epoch [5/10], Step [100/938], Loss: 0.0062\n",
      "Epoch [5/10], Step [200/938], Loss: 0.0470\n",
      "Epoch [5/10], Step [300/938], Loss: 0.0091\n",
      "Epoch [5/10], Step [400/938], Loss: 0.0182\n",
      "Epoch [5/10], Step [500/938], Loss: 0.0084\n",
      "Epoch [5/10], Step [600/938], Loss: 0.0096\n",
      "Epoch [5/10], Step [700/938], Loss: 0.0048\n",
      "Epoch [5/10], Step [800/938], Loss: 0.0081\n",
      "Epoch [5/10], Step [900/938], Loss: 0.0277\n",
      "Epoch [6/10], Step [100/938], Loss: 0.0061\n",
      "Epoch [6/10], Step [200/938], Loss: 0.0144\n",
      "Epoch [6/10], Step [300/938], Loss: 0.1190\n",
      "Epoch [6/10], Step [400/938], Loss: 0.0052\n",
      "Epoch [6/10], Step [500/938], Loss: 0.0208\n",
      "Epoch [6/10], Step [600/938], Loss: 0.0472\n",
      "Epoch [6/10], Step [700/938], Loss: 0.0270\n",
      "Epoch [6/10], Step [800/938], Loss: 0.0051\n",
      "Epoch [6/10], Step [900/938], Loss: 0.0035\n",
      "Epoch [7/10], Step [100/938], Loss: 0.0333\n",
      "Epoch [7/10], Step [200/938], Loss: 0.0009\n",
      "Epoch [7/10], Step [300/938], Loss: 0.0138\n",
      "Epoch [7/10], Step [400/938], Loss: 0.0121\n",
      "Epoch [7/10], Step [500/938], Loss: 0.0388\n",
      "Epoch [7/10], Step [600/938], Loss: 0.0139\n",
      "Epoch [7/10], Step [700/938], Loss: 0.1693\n",
      "Epoch [7/10], Step [800/938], Loss: 0.0107\n",
      "Epoch [7/10], Step [900/938], Loss: 0.0020\n",
      "Epoch [8/10], Step [100/938], Loss: 0.0217\n",
      "Epoch [8/10], Step [200/938], Loss: 0.0251\n",
      "Epoch [8/10], Step [300/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [400/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [500/938], Loss: 0.0277\n",
      "Epoch [8/10], Step [600/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [700/938], Loss: 0.0011\n",
      "Epoch [8/10], Step [800/938], Loss: 0.0041\n",
      "Epoch [8/10], Step [900/938], Loss: 0.0215\n",
      "Epoch [9/10], Step [100/938], Loss: 0.0126\n",
      "Epoch [9/10], Step [200/938], Loss: 0.0277\n",
      "Epoch [9/10], Step [300/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [400/938], Loss: 0.0033\n",
      "Epoch [9/10], Step [500/938], Loss: 0.0125\n",
      "Epoch [9/10], Step [600/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [700/938], Loss: 0.0179\n",
      "Epoch [9/10], Step [800/938], Loss: 0.0043\n",
      "Epoch [9/10], Step [900/938], Loss: 0.0056\n",
      "Epoch [10/10], Step [100/938], Loss: 0.0009\n",
      "Epoch [10/10], Step [200/938], Loss: 0.0052\n",
      "Epoch [10/10], Step [300/938], Loss: 0.0224\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0375\n",
      "Epoch [10/10], Step [500/938], Loss: 0.0151\n",
      "Epoch [10/10], Step [600/938], Loss: 0.0183\n",
      "Epoch [10/10], Step [700/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [800/938], Loss: 0.0040\n",
      "Epoch [10/10], Step [900/938], Loss: 0.0062\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                  .format(epoch+1, 10, i+1, len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.86 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy: {} %'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
